# Entender HTTP ()

## ¿Qué es HTTP?

*Hypertext Transfer Protocol*

Protocolo por el cual dos computadoras se comunican.

- Cliente: realiza una petición
- Servidor: responde la petición (mv de los datacenters)

```http
# Request
GET / HTTP/1.1
Host: developer.mozilla.org  Accept-Language: fr

# Response  HTTP/1.1 200 OK
Date: Sat, 09 Oct 2010 14:28:02 GMT
Server: Apache
Last-Modified: Tue, 01 Dec 2009 20:18:22 GMT  ETag: "51142bc1-7449-479b075b2891b"
Accept-Ranges: bytes  Content-Length: 29769  Content-Type: text/html
<!DOCTYPE html... (here comes the 29769 bytes of the  requested web page)
```

## Códigos de estado de respuesta HTTP
Los códigos de estado de respuesta HTTP indican si se ha completado satisfactoriamente una solicitud HTTP específica. Las respuestas se agrupan en cinco clases:

- Respuestas informativas (100–199),
- Respuestas satisfactorias (200–299),
- Redirecciones (300–399),
- Errores de los clientes (400–499),
- Errores de los servidores (500–599).

# HTML

## ¿Qué es HTML?

HTML es una lenguaje que permite definir la estructura de una página web.
Estrucutra, estilo, partes interactivas. En el contexto de webscraping HTML es muy importante.

Etiquetas está encerrado en angle brakets.<>
Una etiqueta peude contener a otras etiquetas, las etiquetas tienen atributo.

El conocimiento de los atributos será crucial a la hora de realizar scraping porque con ellos podremos conectar el scraper para extraer información.

**Ejemplo de etiquetas:**

```<script>``` hace referencia a un código ejecutable

```<meta>``` aporta información extra al documento (metadatos)

```<iframe>``` colocar paginas externas dentro de la página actual

# Robots.txt
Los archivos robots.txt existen como una forma de administrar una página web. Proporciona información a los rastreadores de los buscadores sobre las páginas o los archivos que pueden solicitar o no de tu sitio web. Principalmente, se utiliza para evitar que tu sitio web se sobrecargue con solicitudes.

En el contexto de webscraping, le dice al scraper que puede y no extraer. Es decir hasta donde puede llegar. Ya que infrigir en la violación de estas directivas puede acarrear un problema legal con el sitio web al que estamos scrapeando.

Contiene entre otros elementos:

- USER-AGENT: Identificadores de quienes acceden a tu sitio web, puede ser un archivo.py hasta un googlebot.

[DIRECTIVAS](https://developers.google.com/search/docs/advanced/robots/create-robots-txt?hl=es&visit_id=637880186130788726-2524145601&rd=1)

- ALLOW: Utiliza esta directiva para permitir a los motores de búsqueda rastrear un subdirectorio o una página, incluso en un directorio que de otro modo no estaría permitido
- DISALLOW: Utiliza esta directiva para indicar a los motores de búsqueda que no accedan a archivos y páginas que se encuentren bajo una ruta específica